 
In this report, we had the opportunity of working with a large dataset that put at our disposal, many alternatives for solving various classification and clustering tasks.\\
In \textbf{Chapter 1} we performed data understanding, cleaning and features/dataset engineering.\\
In \textbf{Chapter 2} analyzed the negative effects that imbalanced dataset have on classification tasks, and we then improved the performances of simple classifiers such as Decision Tree and KNN, by applying and comparing various imbalanced techniques (Random Undersampling, CNN, Tomek's Link, Random Oversampling, SMOTE, K-Means SMOTE, ADASYN and Cost Matrix). We then discovered and removed the top 1\% of anomalies present in our dataset (using DBSCAN, LOF, ABOD, Isolation Forest, Extended Isolation Forest and Autoencoders), and observed improvements in the classification metrics.\\ In \textbf{Chapter 3} we applied advanced classification methods such as Naive Bayes Classifier, Multi-Layer Perceptron, Linear and Non-Linear SVM, RIPPER rule-based classifier, Univariate and Multivariate Linear and Logistic Regression, and Finally Ensemble methods ( Random Forest, Bagging and Adaboost). \\
In \textbf{Chapter 4}, we built a time series dataset from scratch, by extracting the spectral centroid from raw mp3 data. We then applied K-Means clustering (experimenting with Euclidean and DTW distances) and we then identified the top 10 motifs and top 5 anomalies of songs written by the singer "C-Doc".
Finally, we extracted the top 5 shapelets, and based on those, we trained a Decision Tree and a KNN classifier. \\
In \textbf{Chapter 5} instead, we transformed the time series dataset into a sequential dataset and we performed sequential pattern mining. We also extracted frequent sequences of tags associated to the singer The Impossebulls span over a period of 3 years.
We also compared the OPTICS and DBSCAN clustering algorithms and highlighted their differences in terms of clustering performance. We built a transactional dataset from scratch exploiting the information provided by the raw datasets, and performed the K-Modes clustering. 
As last task, we utilized the LIME explainer to provided a clear explanation of why the Random Forest described in Chapter 3 output a certain class label.\\
Overall this project enhanced our reasoning and analytic skills, but also consolidated the theoretical concepts explored in the course.